# [[python_package_import_name]]

Made with Dialogy Template with Simple Transformers

## Features

1.  XLMRWorkflow uses "xlm-roberta-base" for both classification and ner tasks.
2.  Flask integration.
3.  Sentry integration.

## Directory Structure

| File                                      | Description                                                                  |
| ----------------------------------------- | ---------------------------------------------------------------------------- |
| **config**                                | A directory that contains `yaml` files.                                      |
| **data**                                  | Version controlled by `dvc`.                                                 |
| **data/0.0.0**                            | A directory that would contain these directories: datasets, metrics, models. |
| **[[python_package_import_name]]/dev**   | Programs not required in production.                                         |
| **[[python_package_import_name]]/src**   | Programs required in production.                                             |
| **[[python_package_import_name]]/utils** | Programs that offer assitance in either dev or src belong here.              |
| **CHANGELOG.md**                          | Track changes in the code, datasets, etc.                                    |
| **Dockerfile**                            | Containerize the application for production use.                             |
| **LICENSE**                               | Depending on your usage choose the correct copy, don't keep the default!     |
| **Makefile**                              | Helps maintain hygiene before deploying code.                                |
| **pyproject.toml**                        | Track dependencies here. Also, this means you would be using poetry.         |
| **README.md**                             | This must ring a bell.                                                       |
| **uwsgi.ini**                             | Modify as per use.                                                           |

## Getting started

Make sure you have `git`, `python==^3.8`, [`poetry`](https://python-poetry.org/docs/#installation) installed. Preferably within a virtual environment.

### 1. Boilerplate

To create a project using this template, run:

```shell
pip install dialogy
dialogy create hello-world dialogy-template-simple-transformers
make lint
```

The above initiates an interactive session.
![scaffolding_screenshots](./images/dialogy_template_create_command_guide.png)

The questions here help: 

-   Populate your [`pyproject.toml`](https://python-poetry.org/docs/pyproject/) since we use [`poetry`](https://python-poetry.org/docs/) for managing dependencies.
-   Create a repository and python package with the scaffolding you need.

### 2. Install Dependencies

```shell
cd [[python_package_import_name]]
poetry install
```

### 3. Version control

We use [`dvc`](https://dvc.org/doc/install) for dataset and model versioning. 
s3 is the preferred remote to save project level data that are not fit for tracking via git.

The `poetry install` step takes care of dvc installation.

```shell
git init
dvc init
dvc remote add -d s3remote s3://bucket-name/path/to/dir
poetry run [[python_package_import_name]] data --version=0.0.0
dvc add data
```

[% if use_ner %]
This will create a data directory with the following structure:

```shell
data
+---0.0.0
    +---classification
    |   +---datasets
    |   +---metrics
    |   +---models
    +---ner
        +---datasets
        +---metrics
        +---models
```
[% else %]
This will create a data directory with the following structure:

```shell
data
+---0.0.0
    +---classification
        +---datasets
        +---metrics
        +---models
```
[% endif %]

[% if use_ner %]
It is evident that this template concerns itself with only `classification` and `ner` tasks.
[% else %]
It is evident that this template concerns itself with only `classification` tasks.
[% endif %]
You'd typically move your datasets into the datasets directory. The dataset should be split beforehand into `train.csv` and `test.csv`. This template expects these files to be named this way.

The format for classification task `train.csv` is:

```python
In [1]: df[["data", "labels"]].sample(40)
```

|       | data                                              | labels                 |
| ----- | ------------------------------------------------- | ---------------------- |
| 7359  | {"alternatives": {"transcript": "..."             | status                 |
| 19337 | {"alternatives": {"am_score": -182.39217, "c..."  | stop_payment           |
| 903   | {"alternatives": {"transcript": "...", "confi..." | confirm                |
| 15473 | {"alternatives": {"transcript": "..."             | query_loan             |
| 18133 | {"alternatives": {"transcript": "..."             | request_statement      |
| 18954 | {"alternatives": {"am_score": -718.3479, "co..."  | request_statement      |
| 2047  | {"alternatives": {"am_score": -440.15454, "c..."  | account_status         |
| 16159 | {"alternatives": {"transcript": "..."             | request_agent          |
| 13193 | {"alternatives": {"am_score": -702.66943, "c..."  | limit                  |
| 4359  | {"alternatives": {"am_score": -556.99493, "c..."  | branch_address_readout |
| 9561  | {"alternatives": {"am_score": -479.57098, "c..."  | ifsc_code_readout      |
| 4084  | {"alternatives": {"am_score": -304.17725, "c..."  | branch_address_readout |
| 15437 | {"alternatives": {"transcript": "..."             | query_loan             |
| 19543 | {"alternatives": {"am_score": -182.39217, "c..."  | stop_payment           |

The columns named `data` and `labels` are necessary for the classification datasets. Other arbitrary columns
remain unused if given.

A single instance in the `data` column for classification tasks should look like:

```json
{
    "alternatives": [[
        {"transcript": "I need a chequebook",
            "confidence": 0.9581535,
            "am_score": -364.68695,
            "lm_score": 121.946655},
        {"transcript": "I need a cheque book",
            "confidence": 0.958522,
            "am_score": -362.57028,
            "lm_score": 123.00037}]],
    "context": "...""
}
```

---

[% if need_ner %]
And an example for the ner task dataset is:

```python
In [2]: df[df["labels"] != "O"].sample(10)
```

|        | sentence_id | words  | labels         |
| ------ | ----------- | ------ | -------------- |
| 79854  | 14114       | a      | O              |
| 47782  | 10007       | credit | B-product_kind |
| 37038  | 7785        | still  | O              |
| 16142  | 5511        | card   | I-product_kind |
| 13709  | 5205        | want   | O              |
| 114028 | 18547       | just   | O              |
| 61436  | 12006       | offers | B-properties   |
| 106472 | 17546       | my     | O              |
| 91673  | 15458       | i      | O              |
| 85928  | 14710       | other  | O              |

The columns named `sentence_id`, `words`, `labels` are necessary for NER datasets. Other arbitrary columns
remain unused if given.
[% endif %]

### 4. Training

Before training, your directory tree should look like:
[% if use_ner %]
```shell
data
+---0.0.0
    +---classification
    |   +---datasets
    |   |   +---train.csv
    |   |   +---test.csv
    |   +---metrics
    |   +---models
    +---ner
        +---datasets
        |   +---train.csv
        |   +---test.csv
        +---metrics
        +---models
```
[% else %]
```shell
data
+---0.0.0
    +---classification
        +---datasets
        |   +---train.csv
        |   +---test.csv
        +---metrics
        +---models
```
[% endif %]


[% if use_ner %]
These commands help in training the classifier and the NER model.
Specifying the model name in the command will train only the mentioned model.
```shell
poetry run [[python_package_import_name]] train [--version=<version>]
poetry run [[python_package_import_name]] train classification [--version=<version>] # trains only classifier.
poetry run [[python_package_import_name]] train ner [--version=<version>] # trains only NER.
```
[% else %]
These commands help in training the classifier model.
```shell
poetry run [[python_package_import_name]] train [--version=<version>]
```
[% endif %]

Once the training is complete, you would notice the models would be populated:

[% if use_ner %]
```shell
data
+---0.0.0
    +---classification
    |   +---datasets
    |   |   +---train.csv
    |   |   +---test.csv
    |   +---metrics
    |   +---models
    |       +---config.json
    |       +---eval_results.txt
    |       +---labelencoder.pkl
    |       +---model_args.json
    |       +---pytorch_model.bin
    |       +---sentencepiece.bpe.model
    |       +---special_tokens_map.json
    |       +---tokenizer_config.json
    |       +---training_args.bin
    |       +---training_progress_scores.csv
    +---ner
        +---datasets
        |   +---train.csv
        |   +---test.csv
        +---metrics
        +---models
            +---config.json
            +---entity_label_list.pkl
            +---eval_results.txt
            +---model_args.json
            +---pytorch_model.bin
            +---sentencepiece.bpe.model
            +---special_tokens_map.json
            +---tokenizer_config.json
            +---training_args.bin
```
[% else %]
```shell
data
+---0.0.0
    +---classification
        +---datasets
        |   +---train.csv
        |   +---test.csv
        +---metrics
        +---models
            +---config.json
            +---labelencoder.pkl
            +---eval_results.txt
            +---model_args.json
            +---pytorch_model.bin
            +---sentencepiece.bpe.model
            +---special_tokens_map.json
            +---tokenizer_config.json
            +---training_args.bin
            +---training_progress_scores.csv
```
[% endif %]

### 5. Evaluation

To evaluate use:

[% if use_ner %]
```shell
poetry run [[python_package_import_name]] test [--version=<version>]
poetry run [[python_package_import_name]] test classification [--version=<version>]
poetry run [[python_package_import_name]] test ner [--version=<version>]
```
[% else %]
```shell
poetry run [[python_package_import_name]] test [--version=<version>]
```
[% endif %]
(If the version argument is not provided, a default value is used from the _config/config.yaml_.)

Evaluation scripts save the reports in the `metrics` directory.

[% if use_ner %]
```shell
data
+---0.0.0
    +---classification
    |   >---datasets # collapsed
    |   +---metrics
    |   |   +---report.csv
    |   >---models # collapsed
    +---ner
        +---datasets
        |   +---train.csv
        |   +---test.csv
        +---metrics
        |   +---report.csv
        >---models # collapsed
```
[% else %]
```shell
data
+---0.0.0
    +---classification
        +---datasets
        |   +---train.csv
        |   +---test.csv
        +---metrics
        |   +---report.csv
        >---models # collapsed
```
[% endif %]

### 6. Interactive Session

To run your models to see how they perform on live inputs, use the following command:

```shell
poetry run [[python_package_import_name]] repl
```

This prints a set of expected input formats, **if nothing matches, it assumes the input to be plain-text!**
**Make sure you press ESC then ENTER to submit**.

This interface accepts multiline input and takes most people off-guard as they lie waiting for a response.

The reason for putting in a multiline input is to offer convenience over pasting large json request bodies encountered in production and need to be tested locally for debugging.

### 7. Releases

The project comes with an opinion on data management. The default branch (main/master) is expected to contain only the latest version of datasets and models.
The process creates a git tag with the semver so that you can checkout the tag for working on it in isolation to the rest of the project.

To initiate a release process, perform:

```shell
poetry run [[python_package_import_name]] release --version=<version>
```

### 8. Serving

This template also installs [`Flask`](https://flask.palletsprojects.com/en/1.1.x/) for serving APIs. Use its standard documentation for setting up API endpoints. Use `uwsgi` for production use.

```shell
uwsgi --http :9002 --enable-threads --single-interpreter --threads 1 --callable=app --module [[python_package_import_name]].src.api.endpoints:app --ini uwsgi.ini
# You can use any other port, 9002 was only meant as an example.
```

**Do note, the default Flask server is not meant for production!**. Use [`uwsgi`](https://www.digitalocean.com/community/tutorials/how-to-serve-flask-applications-with-uswgi-and-nginx-on-ubuntu-18-04) instead.

## Commands
The CLI commands can be seen [here](./[[python_package_import_name]]/dev/cli.py.jinja)

## Config

```yaml
project_name: [[python_package_import_name]]
version: 0.0.0 # Default version
cores: 8 # no of cpu cores to use for pre-processing.
tasks:
  classification:
    use: true # Use classifier model only if this is True.
    format: "csv" # Either of csv/sqlite.
    threshold: 0.1 # confidence threshold.
    labels: [] # Expected labels, auto-populates the first time a dataset is given.
    alias: # Label aliases
        label_alias_1: an_original_label
        label_alias_2: an_original_label
        label_alias_3: another_original_label
        label_alias_4: yet_another_original_label
    model_args:
      train: {} # These are model arguments for
      prod: {}  # simpletansformers models
      test: {}  # refer to description.
  ner:
    use: false # prediction will not invoke ner predictions.
    format: "csv" # Either of csv/sqlite.
    threshold: 0.6 # confidence threshold.
    labels: [] # Expected labels, auto-populates the first time a dataset is given.
    model_args:
      train: {} # These are model arguments for
      prod: {}  # simpletansformers models
      test: {}  # refer to description.
rules:
  slots: # Relationship shows intents that can contain certain entities.
    intent_a: # contains entity_x and entity_y within slots a-slot-name
        entity_x: # and other-slot-name respectively.
            slot_name: "a-slot-name"
            entity_type: "x-type"
        entity_y:
            slot_name: "other-slot-name"
            entity_type: "y-type"
    intent_b: # contains only entity_y within another-slot-name.
        entity_y:
            slot_name: "another-slot-name"
            entity_type: "y-type"
# Other intents don't contain entities so their slots will be empty.
```

Model args help maintain the configuration of models in a single place, [here](https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model) is a full list, for classification or NER model configuration.

## APIs

These are the APIs which are being used. Some of these are not needed in production.

1.  Health check - To check if the service is running.

    ```python
    @app.route("/", methods=["GET"])
    def health_check():
        return jsonify(
            status="ok",
            response={"message": "Server is up."},
        )
    ```

2.  Predict - This is the main production API.

    ```python
    @app.route("/predict/<lang>/[[python_package_import_name]]/", methods=["POST"])
    ```

## Customization

The best place to setup custom code is the `src` dir. The existing `workflow` would
usually be modified to have api level changes. The API itself can be modified via `api/endpoints.py`.

To modify configuration edit `config/config.yaml`.

## Upcoming Features

-   Stockholm
-   Data conflict
-   Data summarization
-   Visualization and interpretability
